---
title: "Homework 4"
author: "Vitaly Druker"
date: "10/14/2020"
output:
  pdf_document: default
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(faraway)
library(ggplot2)
```

```{r}
# Columns, the total of males and females and the proportion of males
d <- turtle %>% 
  mutate(total = male + female,
         prop = male/total)

d %>% glimpse()
```


```{r}
d %>% 
  ggplot(aes(x = temp, y = prop)) +
  geom_point()
```

- Appears to be a relationship, whether it's linear or not. 

## b

```{r}
# Fit a binomial model
mod_b <- glm(cbind(male, female) ~ temp, data = d, family = binomial)
summary(mod_b) # The temp estimate of 2.2 means is not an odds at this point so we need to exp
exp(2.2) # Gives 9.02 meaning that as the temperature increases by 1 degree, the odds that it’s a male increases by 9. 
# Notice that the temperature range is really small, and it is reasonable that 1 degree of temperature could change the odds by 9
pchisq(deviance(mod_b), df.residual(mod_b), lower.tail = FALSE)
# Under the assumption that mod_b is correct, the deviance is chi sq distributed. So a low p-values invalidates that assumption and that the model is not a good fit.
```

- The p-value is .02, meaning that the model is not a good fit, that the assumption that this follows the binomial distribution is violated at alpha level 0.05
- This says that something is wrong with the model, something doesn’t fit
- This is different from comparing two different models, while you can compare two models in Bernoulli, you can’t measure goodness of fit. But you can in binomial because the deviance is approximately chi-squared with the residual df.

```{r}
# The predications from the model
d$pred_mod_b <- predict(mod_b, d, type = "response")

# Make fake data to make the model line more smooth
d_fake_data <- data.frame(temp = seq(27.2, 29.9, length.out = 100))
d_fake_data$pred_mod_b <- predict(mod_b, d_fake_data, type = "response")

# Overlay the model with the predictions
d %>% 
  ggplot(aes(x = temp, y = prop)) +
  geom_point() +
  # update the y that we're plotting, saying use this other dataset and use this plot for y
  geom_smooth(method = "glm", method.args = list(family = "quasibinomial")) +
  geom_line(data = d_fake_data, aes(y = pred_mod_b), linetype = "longdash")
```

- The smooth function is showing the confidence interval, though the term is se. Don’t use this for inference because you don’t know the model
- Quasibinomial allows you to have proportions in the response. binomial is a count, either 1 or 0 if Bernoulli or cbind for successes and failures

```{r}
# Say if you have a quadratic, on fake data
test_data <- data.frame(x = rnorm(1000, mean = 10))
test_data$y <- test_data$x^2

# Model it, the quadratic is significant, saying that x is a good predator of the model
# However we know that's making the assumption that x is linear to y, but we know that it follows a quadratic distribution
lm(y ~ x, data = test_data) %>% summary()
```

- What’s the difference between testing the goodness of fit test and the model contrast to the null model? Goodness of fit is actually testing if a predictor is valid or not, are all predictors better than no predictors
- Different between: does thus model agrees with the distribution vs. does this predictor valuable addition to better accurately estimate y
- Say something that follows a quadratic equation

## c

- Look at why the deviance is not following the appropriate chi-square distribution
- One reason is if we have spare data, because the Bernoulli does not follow the chi-square distribution therefore if we have sparce data it's very close to Bernoulli
- Though we have low counts, we have more than 5 in each of them
- Looking at counts per trial
- Sparce means that there aren’t a lot of trial, the most sparce data is Bernoulli 
- Here we're saying there were 6 trials at temp 27, different from 1 trial with 30 people at temperature 27. That’s what's being taken into account when we talk about sparsity

```{r}
d %>% 
  ggplot(aes(x = total)) +
  geom_histogram()
```

The data is not sparse, because it has values greater than 5
- to check 

```{r}
halfnorm(residuals(mod_b, type = "pearson"))
```

```{r}
d <- d %>% 
  mutate(emp_logit = log((male + .5)/ (female = .5))) %>% 
  mutate(mod1_pred = predict(mod_b, ., type = "response"))

d %>% 
  ggplot(aes(x = temp,  y = emp_logit)) +
  geom_point()

d %>% 
  ggplot(aes(x = prop, y = mod1_pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "longdash") +
  geom_smooth()

d %>% 
  ggplot(aes(x = temp, y = mod1_pred - prop)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "longdash")
```

## f

```{r}
mod_f <- update(mod_b, . ~ . + I(temp^2))
anova(mod_f, mod_b, test = "Chisq")

pchisq(deviance(mod_f), df.residual(mod_f), lower.tail = F)
```

```{r}
d %>% 
  group_by(temp) %>% 
  summarise(obs_var = var(prop),
         exp_p = sum(male)/sum(male + female),
         total_items = sum(male + female)) %>% 
  ungroup() %>% 
  mutate(exp_var = exp_p*(1-exp_p)/total_items) %>% 
  ggplot(aes(x = obs_var, y = exp_var)) +
  geom_label(aes(label = temp)) +
  geom_abline(slope = 1, intercept = 0, linetype = "longdash") +
  coord_equal(xlim = c(0, .03), ylim = c(0, .03))
```

```{r}
d_grouped <- d %>% 
  group_by(temp) %>% 
  summarise_at(vars(male, female), sum)

mod_h <- glm(cbind(male, female) ~ temp, data = d_grouped, family = binomial)

# glm(cbind(male, female) ~ temp, data = d_grouped, family = quasibinomial) %>% summary()

library(broom)
tidy(mod_h, conf.int = T) %>% 
  left_join(tidy(mod_h, conf.int = T), by = "term")

bind_rows(
  glance(mod_b),
  glance(mod_h)
)

pchisq(deviance(mod_b), df.residual(mod_b), lower.tail = FALSE)
pchisq(deviance(mod_h), df.residual(mod_h), lower.tail = FALSE)

```



