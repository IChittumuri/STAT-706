---
title: "PIMA Dataset"
author: "Vitaly Druker"
date: "9/24/2020"
output:
  pdf_document: default
---

## Lab with `pima` dataset (question 3 of exercise)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

```{r}
d <- rpart::kyphosis
```

```{r}

d %>% glimpse

?rpart::kyphosis
```

```{r}
summary(d)
```

```{r}
# Histograms
d %>% 
  ggplot(aes(x = Start)) + 
  geom_histogram()

d %>% 
  ggplot(aes(x = Age)) + 
  geom_histogram()

d %>% 
  ggplot(aes(x = Number)) + 
  geom_histogram(bins = 50)
```

The first one shows a pretty flat graph, with that big spike starting at 12.
The second one skews a little bit younger but it’s pretty evenly distributed
The third one shows a little peak and some outliers.

```{r}
# Univariate graphs
d %>% 
  ggplot(aes(x = Start, fill = Kyphosis)) +
  geom_density(alpha = .4)

d %>% 
  ggplot(aes(x = Number, fill = Kyphosis)) +
  geom_density(alpha = .4)

d %>% 
  ggplot(aes(x = Age, fill = Kyphosis)) +
  geom_density(alpha = .4)
```

The first one looks like the higher you start, the more likely you're to have kyphosis
The second one also looks like there's some separation here. Looks like the more vertebrae you involve, the more likely you'll have issues afterwards
The third one looks pretty flat for the people who don’t have kyphosis. But seems to peak over at age 100. Looks like the younger people do better than the people who have aged 100 months.

```{r}
# Correlation of the variables together
d %>% 
  ggplot(aes(x = Age, y = Number)) +
  geom_point() +
  geom_smooth(method = 'loess', formula = y~ x)

d %>% 
  ggplot(aes(x = Age, y = Start)) +
  geom_point() +
  geom_smooth(method = 'loess', formula = y~ x)

d %>% 
  ggplot(aes(x = Number, y = Start)) +
  geom_point() +
  geom_smooth(method = 'loess', formula = y~ x)
```

The first one, no real pattern between age in number, pretty flat
The second one, also pretty flat, maybe a little curve going up but nothing pretty obvious.
The third one makes sense because the further you start down the spine the fewer you can do. it has a nice little negative slope

```{r}
library(broom)
# binomial model
# instead of typing out all the predictors, you can just use period which will take everything that is not already in the response
mod <- glm(Kyphosis ~ ., data = d, family = binomial)

summary(mod)

# This is all on a linear scale
mod %>% 
  tidy()

# This is all on the odds scale
mod %>% 
  tidy(conf.int = T, exponentiate = T)
```

In the summary function, the start has an estimate from 0.206, plus or minus .06 for the std error with a significant p-value
On the odds scale, the start estimate means the increase of one of start decreases the odds. It's 81% of the odds of what we were seeing before.

Start of zero is near the head. so the lower you start, the less likely you are to have kyphosis. But the CI high is at 92%, so if you're only saving .08% what are you losing in the surgery?

```{r}
# Step function
mod2 <- step(mod)

# 
tidy(mod2)
```

Nothing happens in the step trace. The AIC does not go down at all, aka it's best to leave everything in the model. Basically the AIC is the lowest when we don’t subtract anything.

For mod2, our p-values are not significant, but that doesn’t mean it's a bad model.

```{r}
# Add some fitted values to this
# Fitted values give you probability right off the bat, you'd have to specify you don’t want probably
# Not how the predict function works, which gives you the linear parameter that you didn’t have to transform.
d$fitted <- fitted.values(mod)

# Calibration curve
d %>% 
  ggplot(aes(x = fitted, y = as.numeric(Kyphosis == 'present'))) +
  geom_point() +
  geom_smooth() +
  geom_abline(slope = 1, intercept = 0, linetype = "longdash") +
  coord_cartesian(xlim  = c(0,1), y = c(0, 1))
```

This curve looks a little crazy, but we don’t have a lot of data. Ideally, it'd be a little straighter. You can see that the bounds of this curve are pretty wide. And we're actually doing a pretty good job going across the full spectrum here. 

Some other things we would do here is look at the sensitivity and the specificity and things like that to see what we could learn. One other thing that'd be interesting to look at is this relationship that we saw between the number and start. Removing anyone of them wouldn’t work because they are both valuable to the model as we saw in the step fn. 

One other thing to keep in mind, is that it looks like removing the number has a very small effect on AIC, but removing the start has a much larger effect on AIC. That one way to start thinking about which features are  important. For example, what if you were missing a bunch of data for the number, maybe you're okay with that if you have the start variable.

```{r}
?faraway::pima

# To make 0 and 1 into negative and positive
factor(c(0,1,1,0,1), levels = c(1, 0 ), labels = c('pos', 'neg'))
```

