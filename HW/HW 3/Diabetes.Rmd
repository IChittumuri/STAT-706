---
title: "Homework 3"
author: "Isabella Chittumuri"
date: "10/08/2020"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Install packages
library(dplyr)
library(faraway)
library(tidyverse)
```

# Chapter 2 Exercise 2

## The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Indians living near Phoenix. The purpose of the study was to investigate factors related to diabetes. The data may be found in the dataset pima.

```{r}
# Get dataset
data("pima")
?pima
summary(pima)
```

## (a) Create a factor version of the test results and use this to produce an interleaved histogram to show how the distribution of insulin differs between those testing positive and negative. Do you notice anything unbelievable about the plot?

```{r message=FALSE, warning=FALSE}
# Change test 0,1 values into negative and positive
pima$test_factor <- ifelse(pima$test == 0, "negative", "positive")

# Interleaved histogram
pima %>% 
  ggplot(aes(x = insulin, color = test_factor)) +
  geom_histogram(fill = "white") + ggtitle("# of women test positive/negative in relation to C/INC insulin values") 
```

This graph displays the number of women who tested positive/negative for diabetes in relation to their insulin levels, with complete and incomplete cases. This distribution is right-skewed for both positive and negative test results.

The reference range for normal 2-Hour serum insulin levels is 16-166 (mu U/ml). Anything above or below that range could be an indictor to problems with insulin production in the body, which can result in diabetes. It's unbelievable to see that there are over 300 females who have insulin levels of 0 (mu U/ml) tested negative for diabetes.

## (b) Replace the zero values of insulin with the missing value code NA. Recreate the interleaved histogram plot and comment on the distribution.

```{r}
# Change insulin 0 values into NA's
pima2 <- pima %>%
  mutate(insulin = na_if(insulin, "0"))

summary(pima2)
```

```{r message=FALSE, warning=FALSE}
# Interleaved histogram w/o insulin missing values (NA)
pima2 %>% 
  ggplot(aes(x = insulin, color = test_factor)) +
  geom_histogram(fill = "white") + ggtitle("# of women test positive/negative in relation to complete(C) insulin values")
```

This graph shows the relationship between women who tested positive/negative for diabetes and their insulin levels, with only complete cases (without NA's). The distribution is still right-skewed, suggesting that the median is a better measure of spread than the mean. 

Looking at the summary output, dataset pima2 (w/o NA's) has a higher insulin median compared to that of dataset pima (w/ NA's), with a difference of 94.5 (mu U/ml).

## (c) Replace the incredible zeros in other variables with the missing value code. Fit a model with the result of the diabetes test as the response and all the other variables as predictors. How many observations were used in the model fitting? Why is this less than the number of observations in the data frame.

```{r echo = TRUE}
# Change all incredible O predictor values into NA's
# Do not include insulin because already done^
# Do not include pregnant because 0 means never been pregnant, valid
pima2$glucose[pima2$glucose==0] <- NA
pima2$diastolic[pima2$diastolic==0] <- NA
pima2$triceps[pima2$triceps==0] <- NA
pima2$bmi[pima2$bmi==0] <- NA
pima2$diabetes[pima2$diabetes==0] <- NA
pima2$age[pima2$age==0] <- NA

# Take out all NA's from dataset
pima3 <- na.omit(pima2)

# Generalized linear model (GLM) w/ link 'logit' because data is skewed
lmod <- glm(test ~ pregnant + glucose + diastolic + triceps + insulin + bmi + diabetes + age, family = binomial(link = "logit"), data = pima3)
 
# Summary of lmod
summary(lmod)

# Coefficients of lmod
lmod %>% broom::tidy()

# Number of observations in lmod
nobs(lmod, use.fallback=F)
```

There are 392 observations used in this model. This is less than the number of observations in the original dataset because we omitted all missing values from each predictor, except pregnant.

## (d) Refit the model but now without the insulin and triceps predictors. How many observations were used in fitting this model? Devise a test to compare this model with that in the previous question.

```{r}
# GLM w/o insulin and triceps
lmod2 <- glm(test ~ pregnant + glucose + diastolic + bmi + diabetes + age, family = binomial(link = "logit"), data = pima3)

# Summary of lmod2
summary(lmod2)

# Coefficients of lmod2
lmod %>% broom::tidy()

# Number of observations in lmod2
nobs(lmod2, use.fallback=F)
```

```{r}
# Anova test
anova(lmod, lmod2, test = "Chi")
```

There are 392 observations used in this model (lmod2), same as the previous model (lmod). Using the ANOVA test, we see that the p-value is greater than 0.05. This suggests that we can drop insulin and triceps as predictors during the fitting of the model. 

## (e) Use AIC to select a model. You will need to take account of the missing values. Which predictors are selected? How many cases are used in your selected model?

```{r}
# AIC
AIC(lmod)
AIC(lmod2)
```

We selected the second model, with the lowest AIC value of 358.8805. This value means that the second model generated a better fit to the data than the first one. The predictors used were pregnant, glucose, diastolic, bmi, diabetes, and age. In our selected model, we have 392 cases. 

## (f) Create a variable that indicates whether the case contains a missing value. Use this variable as a predictor of the test result. Is missingness associated with the test result? Refit the selected model, but now using as much of the data as reasonable. Explain why it is appropriate to do this.

```{r}
# Any column w/ NA will return 0, complete cases will return 1
# Pima2 is the dataset b4 omitted NA's
pima2$check <-  as.integer(complete.cases(pima2))

# Check 0/1 will return FALSE/TRUE
pima2$complete <- factor(pima2$check)
levels(pima2$complete) <- c("FALSE", "TRUE"); levels(pima2$complete)
```

```{r}
# GLM w/ test as response; check as predictor
lmod_check <- glm(test ~ check, family = binomial(link = "logit"), data = pima2)

# Summary of lmod_check
summary(lmod_check)

# Number of observations in lmod_check
nobs(lmod_check, use.fallback=F)

# Coefficients of lmod_check
lmod_check %>% broom::tidy()
```

The variable check is a column created in pima2 that only have values of 0 (incomplete cases) and 1 (complete cases). Looking at the summary of this model, the intercept has a very significant p-value of 3.51e-07, while check has an insignificant p-value of 0.304. This outcome shows that incomplete cases, or missingness, are not associated with the test result. 

```{r}
# Refit selected model (lmod2)
lmod2_step <- step(lmod2, trace=1)
```

```{r}
# GLM w/ test as response; pregnant, glucose, bmi, diabetes, & age as predictors
lmod3 <- glm(test ~ pregnant + glucose + bmi + diabetes + age, family = binomial(link = "logit"), data = pima3)

# Summary of lmod3
summary(lmod3)

# Number of observations in lmod3
nobs(lmod3, use.fallback=F)

# Coefficients of lmod3
lmod3 %>% broom::tidy()
```
 
After using the step function, we see that the best linear model, with the lowest AIC value of 356.89, uses pregnant, glucose, bmi, diabetes, and age as predictors of the test result. This is appropriate because all the other predictors have little to no significance to the model given the ones already included.

## (g) Using the last fitted model of the previous question, what is the difference in the odds of testing positive for diabetes for a woman with a BMI at the first quartile compared with a woman at the third quartile, assuming that all other factors are held constant? Give a confidence interval for this difference.

```{r}
# Coefficients of lmod3
coef(lmod3)
```

```{r}
# Quartile values
quantile(pima3$bmi, c(.25, .75))

quant_diff <- 37.1 - 28.4; quant_diff 
```

```{r}
# Logodds
x1 <- 1
x2 <- 1
x3 <- 1
x4 <- 1
x5 <- 1

logodd_1 = -9.99207971 + 0.08395301 * x1 + 0.03645776 * x2 + 0.07813866 * x3 + 1.15091285 * x4 + 0.03436036 * x5; logodd_1

x3_d <- x3 + quant_diff 

logodd_2 = -9.99207971 + 0.08395301 * x1 + 0.03645776 * x2 + 0.07813866 * x3_d + 1.15091285 * x4 + 0.03436036 * x5; logodd_2
```

```{r}
# Odds
odd1 <- exp(logodd_1); odd1

odd2 <- exp(logodd_2); odd2
```

```{r}
# Odds ratio
odds_ratio <- odd2/odd1; odds_ratio

# check
odds_ratio_check <- exp(0.07813866 * quant_diff); odds_ratio_check
```

We calculated logodd_1 by adding the estimates of our predictors from the last fitted model. Then we exponentiated this value to get odd1, the odds of a positive test result for diabetes. The value for odd1 is 1.826e-4

The difference between the first quartile BMI value and the third quartile BMI value is 8.25. We used this number to alter the BMI estimate of logodd_2. Then we exponentiated logodd_2 to get odd2, which is 3.603e-4. Then to calculate the odds ratio, we divided odd2 by odd1, giving us a value of 1.974. This value is the increase in the odds of getting a positive test result going from the first BMI quartile to the third BMI quartile. 

```{r}
# 95% Confidence Interval for odds ratio

# Bmi standard error (se) of lmod3
se <- coef(summary(lmod3))[, 2]
bmi_se <- se[4]

# Calculate log(OR) because confidence interval (CI) is calculated in a linear scale
log_odds_ratio <- log(odds_ratio); log_odds_ratio

# Calculate exp(CI) to get CI in an odds ratio scale
CI_lower <- exp(log_odds_ratio - (1.96 * bmi_se * quant_diff)); CI_lower
CI_upper <- exp(log_odds_ratio + (1.96 * bmi_se * quant_diff)); CI_upper
```

If we repeat the experiment, we expect, on average, that 95% of the Confidence Interval (CI) contains the true value of the odds ratio. The lower CI limit is 1.389, while the upper CI limit is 2.804. This interval is very close in range to the calculated odds ratio, which was 1.974. This is all to say that 95% of the CI contains the true value of the increase in the odds ratio of getting a positive test result, going from the first BMI quartile to the third BMI quartile.

## (h) Do women who test positive have higher diastolic blood pressures? Is the diastolic blood pressure significant in the regression model? Explain the distinction between the two questions and discuss why the answers are only apparently contradictory.

```{r}
# Interleaved histogram for diastolic and test_factor
pima3 %>% 
  ggplot(aes(x = diastolic, color = test_factor)) +
  geom_histogram(fill = "white")

# Create two datasets, one only positive test results and the other only negative test results
pima3_pos <- subset(pima3, pima3$test == 1)
pima3_neg <- subset(pima3, pima3$test == 0)

summary(pima3_pos$diastolic)
summary(pima3_neg$diastolic)
```

Women who test positive have slightly higher diastolic blood pressure than women who test negative. Between the two, the absolute difference in mean value is 5.11. Diastolic blood pressure isn't significant to the regression model, because there is very little difference in statistical summary values between positive and negative test results.
  
The distinction between the two questions is that the first one is only looking for if women who test positive have higher diastolic blood pressure than those who test negative, which is yes. But it doesn't take into account what the second question does, which is if this difference is substantial enough to have an impact on the regression model, which is no.  
  
The answers are only apparently contradictory if answered in a yes or no format. However, when you look at the minor numerical differences resulting from the first question, it actually supports the answer in the second.

